{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PH_Toolbox.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o6KZS_zSzZi"
      },
      "source": [
        "### Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn55v3sQS2fG"
      },
      "source": [
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# plot several images\n",
        "def plot_instances(size, instances, labels, figure_size=(8, 10), set_title=False, images_per_row=10):\n",
        "  rows = math.ceil(len(instances) / images_per_row) \n",
        "  fig, axes = plt.subplots(nrows=rows, ncols=images_per_row, figsize=figure_size)\n",
        "  if rows == 1:\n",
        "    for index, instance in enumerate(instances):\n",
        "      plt.subplot(1, images_per_row, index+1)\n",
        "      plt.imshow(instance, cmap='binary')\n",
        "      plt.axis('off')\n",
        "      plt.title(label=labels[index])\n",
        "  else:\n",
        "    for i in range(rows):\n",
        "      for j in range(images_per_row):\n",
        "        if i*images_per_row+j >= len(instances):\n",
        "          empty_image = np.zeros((size, size))\n",
        "          axes[i,j].imshow(empty_image, cmap='binary')\n",
        "        else:\n",
        "          axes[i,j].imshow(instances[i*images_per_row+j].reshape(size, size), cmap='binary')\n",
        "          if set_title:\n",
        "            axes[i,j].title.set_text(labels[i*images_per_row+j])\n",
        "        axes[i,j].axis('off')\n",
        "\n",
        "# another way\n",
        "def plot_instances(n_rows, n_cols, instances, set_title=False, labels=[], figure_size=(10, 8)):\n",
        "  fig, axs = plt.subplots(n_rows, n_cols, figsize=figure_size)\n",
        "  for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(instances[i], cmap='binary')\n",
        "    ax.set_axis_off()\n",
        "    if set_title:\n",
        "      ax.set_title(labels[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa_qMrvQb4Xq"
      },
      "source": [
        "# heatmap\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(conf_mx)\n",
        "plt.show()\n",
        "\n",
        "####################\n",
        "plt.matshow(conf_mx)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qONMuKj0_DB8"
      },
      "source": [
        "### Other tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTuHUrRj_Dfg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gctG2uK8_ioG"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk1d8FBWO1C_"
      },
      "source": [
        "### Missing Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpzC17MgOEM9"
      },
      "source": [
        "# find the numbers of missing value\n",
        "housing.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_4clA-ND5fn"
      },
      "source": [
        "# drop the rows with null values\n",
        "drop_1 = housing.dropna(subset=[\"total_bedrooms\"])\n",
        "\n",
        "# drop thr colomns with too much null values that we don't like\n",
        "drop_2 = housing.drop('total_bedrooms', axis=1)\n",
        "\n",
        "# replace with specific values, don't forget to save for later use in the test set\n",
        "median = housing_new['total_bedrooms'].median()\n",
        "housing_new['total_bedrooms'].fillna(median, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnZl0u3JJ23t"
      },
      "source": [
        "### Text Categories to numbers\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYQMy3BAGG6x"
      },
      "source": [
        "# more suitable for ordinal categories like 'bad', 'good'\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "# take care of [[ ]]\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing[['ocean_proximity']])\n",
        "# check categories\n",
        "ordinal_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aireCogRK9lV"
      },
      "source": [
        "# more suitable for nominal categories like 'typeA', 'typeB'\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "nominal_encoder = OneHotEncoder()\n",
        "# return a SciPy sparse matrix\n",
        "housing_cat_1hot = nominal_encoder.fit_transform(housing[['ocean_proximity']])\n",
        "# or use this code\n",
        "# nominal_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# convert sparse matrix to array\n",
        "housing_cat_1hot.toarray()\n",
        "# check categories\n",
        "nominal_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMMrknbjsmfr"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdECdOdqsrK3"
      },
      "source": [
        "# shift image\n",
        "from scipy.ndimage.interpolation import shift\n",
        "\n",
        "def shift_image(image, dx, dy):\n",
        "    image = image.reshape((28, 28))\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    return shifted_image.reshape([-1])\n",
        "\n",
        "image = X_train[1000]\n",
        "shifted_image_down = shift_image(image, 0, 5) # down 5 pixels\n",
        "shifted_image_left = shift_image(image, -5, 0) # left 5 pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh5kjhvQLyHk"
      },
      "source": [
        "### Feature Scaling\n",
        "As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpgJdlELXd-a"
      },
      "source": [
        "# min-max scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "minmax_scaler = MinMaxScaler()\n",
        "# housing_scale = minmax_scaler.fit(housing_new[['median_income']])\n",
        "housing_scale = minmax_scaler.fit_transform(housing_new[['median_income']])\n",
        "\n",
        "# standardization does not bound values to a specific range, like '0-1'\n",
        "# standardization is 0 mean and unit variance\n",
        "# standardization is less affected by outliers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTQSP-htB1Hb"
      },
      "source": [
        "### Other Tools for Data Preparetion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82NY9VDbB0YL"
      },
      "source": [
        "col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = [housing.columns.get_loc(c) for c in col_names] # get the column indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La7sFC2OGKVy"
      },
      "source": [
        "#### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_0AGjglJbC0"
      },
      "source": [
        "# PCA (Principal Component Analysis)\n",
        "'''\n",
        "PCA identifies the axis that accounts for the largest amount of variance in\n",
        "the training set.\n",
        "\n",
        "Reducing dimensionality does cause some information loss (just like compressing an\n",
        "image to JPEG can degrade its quality), so even though it will speed up training, it\n",
        "may make your system perform slightly worse. It also makes your pipelines a bit\n",
        "more complex and thus harder to maintain. So, if training is too slow, you should\n",
        "first try to train your system with the original data before considering using\n",
        "dimensionality reduction. In some cases, reducing the dimensionality of the training\n",
        "data may filter out some noise and unnecessary details and thus result in higher\n",
        "performance, but in general it won’t; it will just speed up training.\n",
        "\n",
        "PCA assumes that the dataset is centered around the origin. As we will see, Scikit-\n",
        "Learn’s PCA classes take care of centering the data for you. If you implement PCA\n",
        "yourself, or if you use other libraries, don’t forget to center the data first.\n",
        "'''\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "# Scikit-Learn’s PCA class uses SVD (Singular Value Decomposition) decomposition to implement PCA\n",
        "pca = PCA(n_components = 2)\n",
        "X2D = pca.fit_transform(X)\n",
        "# The ratio indicates the proportion of the dataset’s variance that\n",
        "# lies along each principal component\n",
        "pca.explained_variance_ratio_\n",
        "\n",
        "# Choose the right dimension\n",
        "# The following code performs PCA without reducing dimensionality, then\n",
        "# computes the minimum number of dimensions required to preserve 95%\n",
        "# of the training set’s variance, You could then set n_components=d and run PCA again.\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "\n",
        "# But there is a much better option: instead of specifying the number of principal\n",
        "# components you want to preserve, you can set n_components to be a float\n",
        "# between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "\n",
        "# Yet another option is to plot the explained variance as a function of the\n",
        "# number of dimensions\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(cumsum, linewidth=3)\n",
        "plt.axis([0, 400, 0, 1])\n",
        "plt.xlabel(\"Dimensions\")\n",
        "plt.ylabel(\"Explained Variance\")\n",
        "plt.plot([d, d], [0, 0.95], \"k:\")\n",
        "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
        "plt.plot(d, 0.95, \"ko\")\n",
        "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
        "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
        "plt.grid(True)\n",
        "# save_fig(\"explained_variance_plot\")\n",
        "plt.show()\n",
        "\n",
        "# It is also possible to decompress the reduced dataset back.\n",
        "# This won’t give you back the original data, since the projection lost a bit\n",
        "# of information (within the 5% variance that was dropped), but it will\n",
        "# likely be close to the original data. The mean squared distance between the\n",
        "# original data and the reconstructed data (compressed and then\n",
        "# decompressed) is called the reconstruction error.\n",
        "pca = PCA(n_components = 154)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "X_recovered = pca.inverse_transform(X_reduced)\n",
        "\n",
        "# By default, svd_solver is actually set to \"auto\": Scikit-Learn\n",
        "# automatically uses the randomized PCA algorithm if m or n is greater than\n",
        "# 500 and d is less than 80% of m or n, or else it uses the full SVD approach.\n",
        "# If you want to force Scikit-Learn to use full SVD, you can set the\n",
        "# svd_solver hyperparameter to \"full\". If you set the svd_solver hyperparameter to \n",
        "# \"randomized\", Scikit-Learn uses a stochastic algorithm called Randomized PCA that \n",
        "# quickly finds an approximation of the first d principal components. Its computational\n",
        "# complexity is O(m × d ) + O(d ), instead of O(m × n ) + O(n ) for the full\n",
        "# SVD approach, so it is dramatically faster than full SVD when d is much smaller than n.\n",
        "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
        "X_reduced = rnd_pca.fit_transform(X_train)\n",
        "\n",
        "# Incremental PCA (IPCA) algorithms allow you to split the training set into mini-batches and\n",
        "# feed an IPCA algorithm one mini-batch at a time. This is useful for large\n",
        "# training sets and for applying PCA online.\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X_train, n_batches):\n",
        "  inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_reduced = inc_pca.transform(X_train)\n",
        "\n",
        "# Kernel PCA (kPCA) \n",
        "'''\n",
        "KPCA making it possible to perform complex nonlinear projections for \n",
        "dimensionality reduction. It is often good at preserving clusters\n",
        "of instances after projection, or sometimes even unrolling datasets that lie\n",
        "close to a twisted manifold.\n",
        "\n",
        "As kPCA is an unsupervised learning algorithm, there is no obvious\n",
        "performance measure to help you select the best kernel and\n",
        "hyperparameter values. That said, dimensionality reduction is often a\n",
        "preparation step for a supervised learning task (e.g., classification), so you\n",
        "can use grid search to select the kernel and hyperparameters that lead to\n",
        "the best performance on that task.\n",
        "\n",
        "Another approach is to select the kernel and hyperparameters that yield the lowest reconstruction error.\n",
        "'''\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "\n",
        "# Find the rigth kernel\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "                (\"kpca\", KernelPCA(n_components=2)),\n",
        "                (\"log_reg\", LogisticRegression())\n",
        "                ])\n",
        "param_grid = [{\n",
        "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
        "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
        "    }]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-fk84rhSasw"
      },
      "source": [
        "# LLE (Locally Linear Embedding)\n",
        "'''\n",
        "LLE works by first measuring how each training instance\n",
        "linearly relates to its closest neighbors (c.n.), and then looking for a lowdimensional\n",
        "representation of the training set where these local\n",
        "relationships are best preserved (more details shortly). This approach\n",
        "makes it particularly good at unrolling twisted manifolds, especially when\n",
        "there is not too much noise.\n",
        "'''\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced = lle.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ACXr-rSycO"
      },
      "source": [
        "# MDS\n",
        "# Reduces dimensionality while trying to preserve the distances between\n",
        "# the instances.\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "X_reduced_mds = mds.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H219bCSUS2rH"
      },
      "source": [
        "# Isomap\n",
        "Creates a graph by connecting each instance to its nearest neighbors,\n",
        "# then reduces dimensionality while trying to preserve the geodesic\n",
        "# distances between the instances.\n",
        "from sklearn.manifold import Isomap\n",
        "\n",
        "isomap = Isomap(n_components=2)\n",
        "X_reduced_isomap = isomap.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh4vFjADS4uB"
      },
      "source": [
        "# TSNE\n",
        "# Reduces dimensionality while trying to keep similar instances close\n",
        "# and dissimilar instances apart. It is mostly used for visualization, in\n",
        "# particular to visualize clusters of instances in high-dimensional space.\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_reduced_tsne = tsne.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FvT4H31MTnX"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2k9xEScMZcX"
      },
      "source": [
        "#### **Predict Values –– Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3zKwnEdjWJQ"
      },
      "source": [
        "'''\n",
        "Based on SVD (Singular Value Decomposition), is more efficient than\n",
        "the NOrmal Equation, plus it handles edge cases like not invertible \n",
        "matrix nicely. But it is not efficient (O(n2)) as the Gradient Descent\n",
        "approach which is better suited for cases where there are a large number of \n",
        "features or too many training instances in fit of memory.\n",
        "'''\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "# Ridge Regression (also called Tikhonov regularization)–– a regularized version of Linear Regression\n",
        "# This forces the learning algorithm to not only fit the data but \n",
        "# also keep the model weights as small as possible.\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])\n",
        "\n",
        "# Lasso Regression (Least Absolute Shrinkage and Selection Operator Regression) \n",
        "# another regularized version of Linear Regression.\n",
        "'''\n",
        "An important characteristic of Lasso Regression is that it tends to eliminate\n",
        "the weights of the least important features (i.e., set them to zero). In other words, \n",
        "Lasso Regression automatically performs feature selection and outputs a sparse model (i.e.,\n",
        "with few nonzero feature weights).\n",
        "'''\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.predict([[1.5]])\n",
        "\n",
        "'''\n",
        "Elastic Net is a middle ground between Ridge Regression and Lasso\n",
        "Regression. The regularization term is a simple mix of both Ridge and\n",
        "Lasso’s regularization terms, and you can control the mix ratio r. When r =\n",
        "0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is\n",
        "equivalent to Lasso Regression.\n",
        "'''\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.predict([[1.5]])\n",
        "\n",
        "# How to choose from plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net?\n",
        "'''\n",
        "It is almost always preferable to have at least a little bit of regularization, \n",
        "so generally you should avoid plain Linear Regression. Ridge is a good default, \n",
        "but if you suspect that only a few features are useful, you should prefer Lasso \n",
        "or Elastic Net because they tend to reduce the useless features’ weights down to \n",
        "zero, as we have discussed. In general, Elastic Net is preferred over Lasso \n",
        "because Lasso may behave erratically when the number of features is greater than the number of\n",
        "training instances or when several features are strongly correlated.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkaogxbJbAKp"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ-_kSsZlJFY"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COQbV_ZlASQO"
      },
      "source": [
        "'''\n",
        "The general idea of Gradient Descent is to tweak parameters iteratively \n",
        "in order to minimize a cost function. You usually start by filling θ with random \n",
        "values (this is called random initialization). When using Gradient Descent, you \n",
        "should ensure that all features have a similar scale, or else it will take much \n",
        "longer to converge.\n",
        "'''\n",
        "\n",
        "# Batch Gradient Descent\n",
        "\n",
        "'''\n",
        "The main problem with Batch Gradient Descent is the fact that it uses the\n",
        "whole training set to compute the gradients at every step, which makes it\n",
        "very slow when the training set is large.\n",
        "'''\n",
        "\n",
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization for weight vector\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "# Visualize Gradient Descent\n",
        "\n",
        "theta_path_bgd = []\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "        theta = theta - eta * gradients\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, 0, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "'''\n",
        "Stochastic Gradient Descent picks a random instance in the training set at every step\n",
        "and computes the gradients based only on that single instance. On the other hand, \n",
        "due to its stochastic (i.e., random) nature, this algorithm is much less regular \n",
        "than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, \n",
        "the cost function will bounce up and down, decreasing only on average. Over time it \n",
        "will end up very close to the minimum, but once it gets there it will continue \n",
        "to bounce around, never settling down. So once the algorithm stops, the final\n",
        "parameter values are good, but not optimal.\n",
        "\n",
        "When the cost function is very irregular, this can actually help the algorithm \n",
        "jump out of local minima, so Stochastic Gradient Descent has a better chance of \n",
        "finding the global minimum than Batch Gradient Descent does.\n",
        "\n",
        "The function that determines the learning rate at each iteration is called the \n",
        "learning schedule. If the learning rate is reduced too quickly, you may get stuck \n",
        "in a local minimum, or even end up frozen halfway to the minimum. If the learning \n",
        "rate is reduced too slowly, you may jump around the minimum for a long time and \n",
        "end up with a suboptimal solution if you halt training too early. \n",
        "\n",
        "If you want to be sure that the algorithm goes through every instance at each epoch, another\n",
        "approach is to shuffle the training set (making sure to shuffle the input\n",
        "features and the labels jointly), then go through it instance by instance, then\n",
        "shuffle it again, and so on. However, this approach generally converges more\n",
        "slowly.\n",
        "\n",
        "When using Stochastic Gradient Descent, the training instances must be independent\n",
        "and identically distributed (IID) to ensure that the parameters get pulled toward the\n",
        "global optimum, on average. A simple way to ensure this is to shuffle the instances\n",
        "during training (e.g., pick each instance randomly, or shuffle the training set at the\n",
        "beginning of each epoch). If you do not shuffle the instances—for example, if the\n",
        "instances are sorted by label—then SGD will start by optimizing for one label, then the\n",
        "next, and so on, and it will not settle close to the global minimum.\n",
        "'''\n",
        "# Use library\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "\n",
        "# Under the hood\n",
        "theta_path_sgd = []\n",
        "m = len(X_b)\n",
        "np.random.seed(42)\n",
        "\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        if epoch == 0 and i < 20:          # only display the first 20 steps        \n",
        "            y_predict = X_new_b.dot(theta)           \n",
        "            style = \"b-\" if i > 0 else \"r--\"       \n",
        "            plt.plot(X_new, y_predict, style)      \n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_sgd.append(theta)              \n",
        "\n",
        "plt.plot(X, y, \"b.\")                              \n",
        "plt.xlabel(\"$x_1$\", fontsize=18)                    \n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)          \n",
        "plt.axis([0, 2, 0, 15])                              \n",
        "plt.show()                                          \n",
        "\n",
        "# Mini-batch Gradient Descent\n",
        "'''\n",
        "At each step, Mini-batch GD computes the gradients on small random sets of \n",
        "instances called mini-batches. The main advantage of Minibatch\n",
        "GD over Stochastic GD is that you can get a performance boost from\n",
        "hardware optimization of matrix operations, especially when using GPUs.\n",
        "\n",
        "The algorithm’s progress in parameter space is less erratic than with\n",
        "Stochastic GD, especially with fairly large mini-batches. As a result, Minibatch\n",
        "GD will end up walking around a bit closer to the minimum than\n",
        "Stochastic GD—but it may be harder for it to escape from local minima (in\n",
        "the case of problems that suffer from local minima, unlike Linear\n",
        "Regression).\n",
        "'''\n",
        "\n",
        "theta_path_mgd = []\n",
        "\n",
        "n_iterations = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size]\n",
        "        yi = y_shuffled[i:i+minibatch_size]\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_mgd.append(theta)\n",
        "\n",
        "# Perform Ridge Regression using Stochastic Gradient Descent.\n",
        "# The penalty hyperparameter sets the type of regularization term to use.\n",
        "# Specifying \"l2\" indicates that you want SGD to add a regularization term to\n",
        "# the cost function equal to half the square of the ℓ norm of the weight vector:\n",
        "# this is simply Ridge Regression. \"l1\" means Lasso Regression.\n",
        "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])\n",
        "\n",
        "# Early stopping example\n",
        "'''\n",
        "With Stochastic and Mini-batch Gradient Descent, the curves are not so smooth, and it\n",
        "may be hard to know whether you have reached the minimum or not. One solution is to\n",
        "stop only after the validation error has been above the minimum for some time (when\n",
        "you are confident that the model will not do any better), then roll back the model\n",
        "parameters to the point where the validation error was at a minimum.\n",
        "'''\n",
        "from copy import deepcopy\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = deepcopy(sgd_reg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJBeYK5Rl08s"
      },
      "source": [
        "# Polynomial Regression\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "# PolynomialFeatures also adds all combinations of features up to the given degree.\n",
        "# Like a3, b3, a2b, ab2\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X) # Expand features\n",
        "\n",
        "lin_reg = LinearRegression() # Use linear regression models to do the polynomial\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QTMQj_fwoFT"
      },
      "source": [
        "# Support vector mahchine (SVM) regression\n",
        "'''\n",
        "To use SVMs for regression instead of classification, the trick is to reverse the objective: \n",
        "instead of trying to fit the largest possible street between two classes while limiting margin\n",
        "violations, SVM Regression tries to fit as many instances as possible on the street while\n",
        "limiting margin violations (i.e., instances off the street). The width of the street is\n",
        "controlled by a hyperparameter, ϵ (called tol in Scikit-Learn, it's like the margin width).\n",
        "\n",
        "The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is the\n",
        "regression equivalent of the LinearSVC class. The LinearSVR class scales linearly with the\n",
        "size of the training set (just like the LinearSVC class), while the SVR class gets much too\n",
        "slow when the training set grows large (just like the SVC class).\n",
        "'''\n",
        "\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
        "svm_reg.fit(X, y)\n",
        "\n",
        "# Kernelized support vector mahchine (SVM) for nonlinear regression\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
        "svm_poly_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXISXV-JI1uv"
      },
      "source": [
        "# Decision tree regression\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
        "tree_reg1.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWGnf5jzXaO6"
      },
      "source": [
        "#### **Predict Class –– Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02dXRb8IXhS9"
      },
      "source": [
        "# SGD Classifier\n",
        "'''\n",
        "This classifier has the advantage of being capable of handling very large \n",
        "datasets efficiently. This is in part because SGD deals with training instances \n",
        "independently, one at a time (which also makes SGD well suited for online learning).\n",
        "Capable of handling multiple classes.\n",
        "'''\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(random_state=168)\n",
        "sgd_clf.fit(X_train, y_train_5)\n",
        "sgd_clf.predict([X[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bp2maZeYGcb"
      },
      "source": [
        "# Decision tree\n",
        "'''\n",
        "Decision Trees are versatile Machine Learning algorithms that can perform both\n",
        "classification and regression tasks, and even multioutput tasks.\n",
        "\n",
        "Decision Trees require very little data preparation. In fact, they\n",
        "don’t require feature scaling or centering at all.\n",
        "\n",
        "Scikit-Learn uses the CART (Classification and Regression Tree) algorithm, which \n",
        "produces only binary trees: nonleaf nodes always have two children (i.e., questions \n",
        "only have yes/no answers). However, other algorithms such as ID3 can produce\n",
        "Decision Trees with nodes that have more than two children.\n",
        "\n",
        "Decision tree has the predict_proba() method.\n",
        "\n",
        "Most of the time entropy and Gini do not make a big difference: they lead to \n",
        "similar trees. Gini impurity is slightly faster to compute, so it is a good default. \n",
        "However, when they differ, Gini impurity tends to isolate the most frequent class \n",
        "in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\n",
        "\n",
        "Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.\n",
        "\n",
        "Decision Trees have some limitations: Decision Trees love orthogonal decision boundaries \n",
        "(all splits are perpendicular to an axis), which makes them sensitive to training set rotation, \n",
        "one way to limit this problem is to use Principal Component Analysis;\n",
        "the main issue with Decision Trees is that they are very sensitive to small\n",
        "variations in the training data (Actually, since the training algorithm used by \n",
        "Scikit-Learn is stochastic, you may get very different models even on the\n",
        "same training data (unless you set the random_state hyperparameter)), \n",
        "random Forests can limit this instability by averaging predictions over many trees.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X, y)\n",
        "\n",
        "# Visualize decision tree\n",
        "\n",
        "from graphviz import Source\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(\n",
        "        tree_clf,\n",
        "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
        "        feature_names=iris.feature_names[2:],\n",
        "        class_names=iris.target_names,\n",
        "        rounded=True,\n",
        "        filled=True\n",
        "    )\n",
        "\n",
        "# convert .dot to .png\n",
        "# $ dot -Tpng iris_tree.dot -o iris_tree.png\n",
        "\n",
        "Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))\n",
        "\n",
        "# I don't want to save these plots, so just show them\n",
        "def draw_tree_graph(tree):\n",
        "    data = export_graphviz(tree, \n",
        "                           feature_names=iris.feature_names, class_names=iris.target_names, \n",
        "                           filled=True, rounded=True\n",
        "                          )\n",
        "    graph = Source(data)\n",
        "    return graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5rglr6ME8IP"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC()\n",
        "svm_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSaJrkxRjRQP"
      },
      "source": [
        "# KNN \n",
        "# can handle multilabel, and multioutput\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train, y_multilabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAOS3UiFAkJQ"
      },
      "source": [
        "# Logistic Regression\n",
        "'''\n",
        "The bad news is that there is no known closed-form equation to compute the\n",
        "value of θ that minimizes this cost function (there is no equivalent of the\n",
        "Normal Equation). The good news is that this cost function is convex, so\n",
        "Gradient Descent (or any other optimization algorithm) is guaranteed to find\n",
        "the global minimum (if the learning rate is not too large and you wait long\n",
        "enough).\n",
        "\n",
        "Just like the other linear models, Logistic Regression models can be\n",
        "regularized using ℓ or ℓ penalties. Scikit-Learn actually adds an ℓ penalty\n",
        "by default.\n",
        "\n",
        "The hyperparameter controlling the regularization strength of a Scikit-Learn\n",
        "LogisticRegression model is not alpha (as in other linear models), but its inverse:\n",
        "C. The higher the value of C, the less the model is regularized.\n",
        "'''\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "log_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzApv6C6IpMt"
      },
      "source": [
        "# Softmax Regression / Multinomial Logiistic Regression\n",
        "'''\n",
        "The Logistic Regression model can be generalized to support multiple\n",
        "classes directly, without having to train and combine multiple binary\n",
        "classifiers.\n",
        "\n",
        "The idea is simple: when given an instance x, the Softmax Regression model\n",
        "first computes a score s (x) for each class k, then estimates the probability of\n",
        "each class by applying the softmax function (also called the normalized exponential) \n",
        "to the scores.\n",
        "\n",
        "Just like the Logistic Regression classifier, the Softmax Regression\n",
        "classifier predicts the class with the highest estimated probability (which is\n",
        "simply the class with the highest score),\n",
        "\n",
        "The Softmax Regression classifier predicts only one class at a time (i.e., it is multiclass,\n",
        "not multioutput), so it should be used only with mutually exclusive classes, such as\n",
        "different types of plants. You cannot use it to recognize multiple people in one picture.\n",
        "\n",
        "Scikit-Learn’s LogisticRegression uses one-versus-the-rest by\n",
        "default when you train it on more than two classes, but you can set the\n",
        "multi_class hyperparameter to \"multinomial\" to switch it to Softmax\n",
        "Regression. You must also specify a solver that supports Softmax\n",
        "Regression, such as the \"lbfgs\" solver (see Scikit-Learn’s documentation\n",
        "for more details). It also applies ℓ regularization by default, which you can\n",
        "control using the hyperparameter C.\n",
        "'''\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)\n",
        "softmax_reg.predict([[5, 2]])\n",
        "softmax_reg.predict_proba([[5, 2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPMr8lLVot73"
      },
      "source": [
        "# Support vector machine (SVM)\n",
        "'''\n",
        "Support vectors are the instances located on the edge.\n",
        "\n",
        "SVMs are sensitive to the feature scales, for example, when the vertical scale is\n",
        "much larger than the horizontal scale, the widest possible street is close to horizontal.\n",
        "\n",
        "It's a binary classifier.\n",
        "'''\n",
        "# linear SVM model (using the LinearSVC class with C=1 and the hinge loss function) for linear classification\n",
        "'''\n",
        "The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its\n",
        "mean. This is automatic if you scale the data using the StandardScaler. Also make sure you set the loss\n",
        "hyperparameter to \"hinge\", as it is not the default value. Finally, for better performance, you should set\n",
        "the dual hyperparameter to False, unless there are more features than training instances.\n",
        "\n",
        "As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than\n",
        "SVC(kernel=\"linear\"). The LinearSVC class is based on the liblinear library, which implements an optimized\n",
        "algorithm for linear SVMs. It does not support the kernel trick, but it scales almost\n",
        "linearly with the number of training instances and the number of features. Its training time\n",
        "complexity is roughly O(m × n)), especially if the training set is very large or if it has plenty of features. If the\n",
        "training set is not too large, you should also try the Gaussian RBF kernel; it works well in most cases.\n",
        "\n",
        "Then if you have spare time and computing power, you can experiment with a few other kernels, using\n",
        "cross-validation and grid search. You’d want to experiment like that especially if there are kernels\n",
        "specialized for your training set’s data structure.\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
        "\n",
        "svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
        "    ])\n",
        "\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "# Instead of using the LinearSVC class, we could use the SVC class with a linear kernel.\n",
        "'''\n",
        "Or we could use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\n",
        "alpha=1/(m*C)). This applies regular Stochastic Gradient Descent to train\n",
        "a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it can be\n",
        "useful to handle online classification tasks or huge datasets that do not fit in memory (outof-\n",
        "core training).\n",
        "'''\n",
        "from sklearn.svm import SVC\n",
        "svm_clf = SVC(kernel=\"linear\", C=1)\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "# linear SVM model for non-linear classification by adding polynomial features\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.15) \n",
        "polynomial_svm_clf = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=3)), \n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
        "    ])\n",
        "\n",
        "polynomial_svm_clf.fit(X, y)\n",
        "\n",
        "# linear SVM model for non-linear classification by poly kernel trick\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "#       The hyperparameter coef0 controls how much the model is influenced by high-degree\n",
        "#       polynomials versus low-degree polynomials.\n",
        "    ])\n",
        "poly_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "# linear SVM model for non-linear classification by similar function - Gaussian RBF\n",
        "'''\n",
        "Just like the polynomial features method, the similarity features method can be useful with\n",
        "any Machine Learning algorithm, but it may be computationally expensive to compute all\n",
        "the additional features, especially on large training sets. So we need kernel trick.\n",
        "\n",
        "Increasing gamma makes the bell-shaped curve narrower. As a result, each\n",
        "instance’s range of influence is smaller: the decision boundary ends up being more\n",
        "irregular, wiggling around individual instances. Conversely, a small gamma value makes the\n",
        "bell-shaped curve wider: instances have a larger range of influence, and the decision\n",
        "boundary ends up smoother. So γ acts like a regularization hyperparameter: if your model\n",
        "is overfitting, you should reduce it; if it is underfitting, you should increase it (similar to\n",
        "the C hyperparameter).\n",
        "'''\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "    ])\n",
        "rbf_kernel_svm_clf.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ki7yBRaUKqt"
      },
      "source": [
        "#### **Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqEr6c1fUh_H"
      },
      "source": [
        "# KMeans\n",
        "# Unsupervised model\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "# Mini Batch KMeans\n",
        "# Faster and suit for out-of-core sataset\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "minibatch_kmeans = MiniBatchKMeans(n_clusters=5)\n",
        "minibatch_kmeans.fit(X)\n",
        "\n",
        "# Find the optimal number of clusters\n",
        "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "                for k in range(1, 10)]\n",
        "inertias = [model.inertia_ for model in kmeans_per_k]\n",
        "\n",
        "plt.figure(figsize=(8, 3.5))\n",
        "plt.plot(range(1, 10), inertias, \"bo-\")\n",
        "plt.xlabel(\"$k$\", fontsize=14)\n",
        "plt.ylabel(\"Inertia\", fontsize=14)\n",
        "plt.annotate('Elbow',\n",
        "             xy=(4, inertias[3]),\n",
        "             xytext=(0.55, 0.55),\n",
        "             textcoords='figure fraction',\n",
        "             fontsize=16,\n",
        "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
        "            )\n",
        "plt.axis([1, 8.5, 0, 1300])\n",
        "# save_fig(\"inertia_vs_k_plot\")\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_scores = [silhouette_score(X, model.labels_)\n",
        "                     for model in kmeans_per_k[1:]]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
        "plt.xlabel(\"$k$\", fontsize=14)\n",
        "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
        "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
        "# save_fig(\"silhouette_score_vs_k_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riFEZuCr9910"
      },
      "source": [
        "### Ensamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV08hrMZ99hF"
      },
      "source": [
        "# Voting classifiers\n",
        "\n",
        "'''\n",
        "Ensemble methods work best when the predictors are as independent from one another as possible.\n",
        "One way to get diverse classifiers is to train them using very different algorithms. This increases the\n",
        "chance that they will make very different types of errors, improving the ensemble’s accuracy.\n",
        "\n",
        "A very simple way to create an even better classifier is to aggregate the predictions of\n",
        "each classifier and predict the class that gets the most votes. This majority-vote\n",
        "classifier is called a hard voting classifier.\n",
        "\n",
        "If all classifiers are able to estimate class probabilities (i.e., they all have a\n",
        "predict_proba() method), then you can tell Scikit-Learn to predict the class with the\n",
        "highest class probability, averaged over all the individual classifiers. This is called soft\n",
        "voting. It often achieves higher performance than hard voting because it gives more\n",
        "weight to highly confident votes. All you need to do is replace voting=\"hard\" with\n",
        "voting=\"soft\" and ensure that all classifiers can estimate class probabilities.\n",
        "'''\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')\n",
        "\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# soft voting\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42) # notice the probability here\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft')\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzmxGmN0_r0q"
      },
      "source": [
        "# Bagging and pasting\n",
        "'''\n",
        "Use the same training algorithm for every\n",
        "predictor and train them on different random subsets of the training set. When sampling\n",
        "is performed with replacement, this method is called bagging (short for bootstrap\n",
        "aggregating ). When sampling is performed without replacement, it is called pasting.\n",
        "In other words, both bagging and pasting allow training instances to be sampled several\n",
        "times across multiple predictors, but only bagging allows training instances to be\n",
        "sampled several times for the same predictor.\n",
        "\n",
        "Predictors can all be trained in parallel, via different CPU\n",
        "cores or even different servers. Similarly, predictions can be made in parallel. This is\n",
        "one of the reasons bagging and pasting are such popular methods: they scale very well.\n",
        "\n",
        "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
        "trained on, so bagging ends up with a slightly higher bias than pasting; but the extra\n",
        "diversity also means that the predictors end up being less correlated, so the ensemble’s\n",
        "variance is reduced. Overall, bagging often results in better models, which explains why\n",
        "it is generally preferred. However, if you have spare time and CPU power, you can use\n",
        "cross-validation to evaluate both bagging and pasting and select the one that works best.\n",
        "'''\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, random_state=42)\n",
        "# if you want to use pasting instead, just set bootstrap=False).\n",
        "# The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training \n",
        "# and predictions (–1 tells Scikit-Learn to use all available cores).\n",
        "# The BaggingClassifier automatically performs soft voting instead of hard voting if the base\n",
        "# classifier can estimate class probabilities.\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "# In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\n",
        "# request an automatic oob evaluation after training. By default a BaggingClassifier samples m\n",
        "# training instances with replacement (bootstrap=True), where m is the size of the\n",
        "# training set. This means that only about 63% of the training instances are sampled on\n",
        "# average for each predictor. The remaining 37% of the training instances that are not\n",
        "# sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for\n",
        "# all predictors. Since a predictor never sees the oob instances during training, \n",
        "# it can be evaluated on these instances, without the need for a separate validation set.\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    bootstrap=True, oob_score=True, random_state=40)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_\n",
        "\n",
        "# The BaggingClassifier class supports sampling the features as well. Sampling is\n",
        "# controlled by two hyperparameters: max_features and bootstrap_features. \n",
        "# This technique is particularly useful when you are dealing with high-dimensional inputs\n",
        "# (such as images). Sampling both training instances and features is called the Random\n",
        "# Patches method. Keeping all training instances (by setting bootstrap=False and\n",
        "# max_samples=1.0) but sampling features (by setting bootstrap_features to True\n",
        "# and/or max_features to a value smaller than 1.0) is called the Random Subspaces\n",
        "# method. Sampling features results in even more predictor diversity, trading a \n",
        "# bit more bias for a lower variance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzZZjD-Htvj"
      },
      "source": [
        "# Random forest\n",
        "# Capable of handling multiple classes.\n",
        "'''\n",
        "A Random Forest is an ensemble of Decision Trees, generally trained via the bagging \n",
        "method (or sometimes pasting), typically with max_samples set to the size of the \n",
        "training set (similarly, there is a RandomForestRegressor class for regression tasks).\n",
        "\n",
        "The Random Forest algorithm introduces extra randomness when growing trees; instead\n",
        "of searching for the very best feature when splitting a node, it searches\n",
        "for the best feature among a random subset of features. The algorithm results in greater\n",
        "tree diversity, which (again) trades a higher bias for a lower variance, generally yielding\n",
        "an overall better model.\n",
        "'''\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "\n",
        "# Scikit-Learn measures a feature’s importance by\n",
        "# looking at how much the tree nodes that use that feature reduce impurity on average\n",
        "# (across all trees in the forest).\n",
        "rnd_clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG1t2R0xVCr_"
      },
      "source": [
        "# Boosting\n",
        "'''\n",
        "Boosting (originally called hypothesis boosting) refers to any Ensemble method that can\n",
        "combine several weak learners into a strong learner. The general idea of most boosting\n",
        "methods is to train predictors sequentially, each trying to correct its predecessor.\n",
        "'''\n",
        "\n",
        "# AdaBoost\n",
        "'''\n",
        "One way for a new predictor to correct its predecessor is to pay a bit more attention to\n",
        "the training instances that the predecessor underfitted. This results in new predictors\n",
        "focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
        "\n",
        "There is one important drawback to this sequential learning technique: it cannot be parallelized (or\n",
        "only partially), since each predictor can only be trained after the previous predictor has been trained\n",
        "and evaluated. As a result, it does not scale as well as bagging or pasting.\n",
        "\n",
        "Scikit-Learn uses a multiclass version of AdaBoost called SAMME (which stands for\n",
        "Stagewise Additive Modeling using a Multiclass Exponential loss function). When there\n",
        "are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate\n",
        "class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use\n",
        "a variant of SAMME called SAMME.R (the R stands for “Real”), which relies on class\n",
        "probabilities rather than predictions and generally performs better.\n",
        "\n",
        "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of\n",
        "estimators or more strongly regularizing the base estimator.\n",
        "'''\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# GRadient boosting\n",
        "'''\n",
        "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
        "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
        "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
        "errors made by the previous predictor.\n",
        "\n",
        "The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
        "which specifies the fraction of training instances to be used for training each tree. For\n",
        "example, if subsample=0.25, then each tree is trained on 25% of the training instances,\n",
        "selected randomly. This technique trades a higher bias for a lower variance. It also \n",
        "speeds up training considerably. This is called Stochastic Gradient Boosting.\n",
        "'''\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
        "gbrt.fit(X, y)\n",
        "\n",
        "# Gradient boost with early stopping 1 \n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)\n",
        "\n",
        "# Gradient boost with early stopping 2\n",
        "# The following code stops training when the validation error does not improve for five iterations in a row:\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break  # early stopping\n",
        "\n",
        "# It is worth noting that an optimized implementation of Gradient Boosting is available in\n",
        "# the popular Python library XGBoost, which stands for Extreme Gradient Boosting. This\n",
        "# package was initially developed by Tianqi Chen as part of the Distributed (Deep)\n",
        "# Machine Learning Community (DMLC), and it aims to be extremely fast, scalable, and\n",
        "# portable. In fact, XGBoost is often an important component of the winning entries in\n",
        "# ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\n",
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)\n",
        "# XGBoost also offers several nice features, such as automatically taking care of early\n",
        "# stopping:\n",
        "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)\n",
        "\n",
        "\n",
        "# Under the hood\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y)\n",
        "\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg2.fit(X, y2)\n",
        "\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg3.fit(X, y3)\n",
        "\n",
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZAluEFcdqnS"
      },
      "source": [
        "# Stacking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKHpTgC2MxL2"
      },
      "source": [
        "### Evaluate "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43g5umLp0Lg_"
      },
      "source": [
        "**Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6BnboKNMyZ6"
      },
      "source": [
        "# mean squared error for regression\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vhNpK69b_LU"
      },
      "source": [
        "# mean squared error for regression with cross validation\n",
        "'''\n",
        "Scikit-Learn’s cross-validation features expect a utility function (greater is better) \n",
        "rather than a cost function (lower is better), so the scoring function is actually \n",
        "the opposite of the MSE (i.e., a negative value), which is why the following code \n",
        "computes -scores before calculating the square root.\n",
        "\n",
        "If GridSearchCV is initialized with refit=True (which is the default), then once it finds\n",
        "the best estimator using cross-validation, it retrains it on the whole training set. This is\n",
        "usually a good idea, since feeding it more data will likely improve its performance.\n",
        "\n",
        "If a model performs well on the training data but generalizes poorly according \n",
        "to the cross-validation metrics, then your model is overfitting. If it performs \n",
        "poorly on both, then it is underfitting.\n",
        "'''\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "tree_rmse_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agg6-iBuUEMU"
      },
      "source": [
        "# 95% confidence interval for the generalization error\n",
        "from scipy import stats\n",
        "\n",
        "confidence = 0.95\n",
        "squared_errors = (final_predictions - y_test) ** 2\n",
        "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
        "                         loc=squared_errors.mean(),\n",
        "                         scale=stats.sem(squared_errors)))\n",
        "\n",
        "# compute the interval manually\n",
        "m = len(squared_errors)\n",
        "mean = squared_errors.mean()\n",
        "tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\n",
        "tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
        "np.sqrt(mean - tmargin), np.sqrt(mean + tmargin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VsRpqBXpFMY"
      },
      "source": [
        "# Check overfitting or underfitting by plot learning curves\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "'''\n",
        "Check the error first.\n",
        "If there is a gap between the curves. This means that the model\n",
        "performs significantly better on the training data than on the\n",
        "validation data, which is the hallmark of an overfitting model. If\n",
        "you used a much larger training set, however, the two curves would\n",
        "continue to get closer.\n",
        "'''\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
        "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
        "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZAnqqv-0OV1"
      },
      "source": [
        "**Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9XWp4M8qcss"
      },
      "source": [
        "# implement cross validation by stratified k fold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import clone\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=3, random_state=168)\n",
        "\n",
        "for train_index, test_index in skfold.split(X_train, y_train_5):\n",
        "  clone_sgd_clf = clone(sgd_clf)\n",
        "  X_train_folds = X_train[train_index]\n",
        "  y_train_folds = y_train_5[train_index]\n",
        "  X_test_folds = X_train[test_index]\n",
        "  y_test_folds = y_train_5[test_index]\n",
        "\n",
        "  clone_sgd_clf.fit(X_train_folds, y_train_folds)\n",
        "  y_pred = clone_sgd_clf.predict(X_test_folds)\n",
        "  n_correct = sum(y_pred == y_test_folds)\n",
        "  print(f'Correct rate is: {n_correct / len(y_pred)}')\n",
        "\n",
        "'''\n",
        "accuracy is generally not the preferred performance measure for classifiers, \n",
        "especially when you are dealing with skewed datasets (i.e., when some classes \n",
        "are much more frequent than others)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yalgNVVkiY_"
      },
      "source": [
        "# confusion matrics\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_train_5, y_train_pred)\n",
        "# true negatives   |  false positives\n",
        "# false negatives  |  true positives\n",
        "# precision = TP / (TP + FP)\n",
        "# recall (sensitivity, true positive rate (TPR)) = TP / (TP + FN)  the ratio of positive instances that are correctly detected by the classifier\n",
        "'''\n",
        "F1 score = 2 * (precision * recall) / (precision + recall) \n",
        "F1 is the harmonic mean of precision and recall. Whereas the regular mean treats \n",
        "all values equally, the harmonic mean gives much more weight to low values. \n",
        "As a result, the classifier will only get a high F1 score if both recall and precision are high.\n",
        "'''\n",
        "# precision/recall trade-off\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(f'precision: {precision_score(y_train_5, y_train_pred)}, recall: {recall_score(y_train_5, y_train_pred)}, f1: {f1_score(y_train_5, y_train_pred)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rysjwF5C1FHU"
      },
      "source": [
        "# PR curve (Precision and recall versus the decision threshold)\n",
        "# help choose the threshold for binary classifier (precision-recall trade-off)\n",
        "'''\n",
        "As a rule of thumb, you should prefer the PR curve\n",
        "whenever the positive class is rare or when you care more about the false positives\n",
        "than the false negatives. Otherwise, use the ROC curve.\n",
        "'''\n",
        "\n",
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')\n",
        "\n",
        "from sklearn.metrics import  precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5,\n",
        "y_scores)\n",
        "\n",
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "    plt.legend(loc=\"center right\", fontsize=16) \n",
        "    plt.xlabel(\"Threshold\", fontsize=16)        \n",
        "    plt.grid(True)                              \n",
        "    plt.axis([-50000, 50000, 0, 1])            \n",
        "\n",
        "recall_90_precision = recalls[np.argmax(precisions >= 0.90)] # get the index of recalls when precision = 0.90\n",
        "# In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\n",
        "# argmax returns the first index when precision >= 0.90 is true \n",
        "# and the precisions is sorted by the threshold\n",
        "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
        "\n",
        "plt.figure(figsize=(8, 4))                                                                  \n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")  # vertical line               \n",
        "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\") # horizontal line 1                               \n",
        "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\") # horizontal liine 2\n",
        "plt.plot([threshold_90_precision], [0.9], \"ro\") # point 1                                            \n",
        "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\") # point 2                            \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bwfbgZ30XKS"
      },
      "source": [
        "# ROC curve and are under the curve(AUC)\n",
        "'''\n",
        "The receiver operating characteristic (ROC) curve is another common\n",
        "tool used with binary classifiers. It plots the true positive rate (another name \n",
        "for recall) against the false positive rate (FPR). The FPR is the ratio of \n",
        "negative instances that are incorrectly classified as positive. \n",
        "It is equal to 1 – the true negative rate (TNR), which is the ratio of negative \n",
        "instances that are correctly classified as negative.\n",
        "The TNR is also called specificity. Hence, the ROC curve plots sensitivity\n",
        "(recall) versus 1 – specificity.\n",
        "The higher the recall (TPR), the more false positives (FPR) the classifier produces.\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function') # a score calculated by the classifier\n",
        "\n",
        "from sklearn.metrics import  precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, label=None):\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal represents the ROC curve of a purely random classifier\n",
        "    plt.axis([0, 1, 0, 1])                                    \n",
        "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
        "    plt.grid(True)                                           \n",
        "\n",
        "plt.figure(figsize=(8, 6))                                   \n",
        "plot_roc_curve(fpr, tpr)\n",
        "recall_90_precision = recalls[np.argmax(precisions >= 0.90)] \n",
        "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]        \n",
        "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")  \n",
        "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\") \n",
        "plt.plot([fpr_90], [recall_90_precision], \"ro\")                                              \n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "'''\n",
        "A perfect classifier will have a ROC AUC equal to 1, whereas a\n",
        "purely random classifier will have a ROC AUC equal to 0.5.\n",
        "'''\n",
        "roc_auc_score(y_train_5, y_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPIJ17pD5ZxS"
      },
      "source": [
        "# ROC and AUC by probability\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(random_state=168)\n",
        "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')\n",
        "\n",
        "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
        "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n",
        "\n",
        "recall_90_precision = recalls[np.argmax(precisions >= 0.90)] \n",
        "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]    \n",
        "recall_for_forest = tpr_forest[np.argmax(fpr_forest >= fpr_90)]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
        "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
        "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")\n",
        "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\n",
        "plt.plot([fpr_90], [recall_90_precision], \"ro\")\n",
        "plt.plot([fpr_90, fpr_90], [0., recall_for_forest], \"r:\")\n",
        "plt.plot([fpr_90], [recall_for_forest], \"ro\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\", fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjXD5UhGAlI5"
      },
      "source": [
        "### Fine-Tune the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Jaf6IVBztN"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "              {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, \n",
        "              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "              ]\n",
        "forest_reg = RandomForestRegressor()\n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
        "# grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
        "grid_search.fit(housing_prepared, housing_labels)\n",
        "\n",
        "grid_search.best_params_\n",
        "grid_search.best_estimator_\n",
        "pd.DataFrame(grid_search.cv_results_)\n",
        "cvres = grid_search.cv_results_\n",
        "\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "  print(np.sqrt(-mean_score), params)\n",
        "\n",
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kutOlHrqNH6i"
      },
      "source": [
        "# similar as GridSearchCV but only need to set the iteration parameter\n",
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nDSTEIm_g7P"
      },
      "source": [
        "### Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ei2Zy3ZWY7l"
      },
      "source": [
        "full_pipeline_with_predictor = Pipeline([\n",
        "        (\"preparation\", full_pipeline),\n",
        "        (\"linear\", LinearRegression())\n",
        "    ])\n",
        "\n",
        "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
        "full_pipeline_with_predictor.predict(some_data)\n",
        "\n",
        "my_model = full_pipeline_with_predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o01zlLT_jwY"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(my_model, \"my_model.pkl\")\n",
        "# and later...\n",
        "my_model_loaded = joblib.load(\"my_model.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RmdZTHJSH_Q"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPlio_Q8OnYn"
      },
      "source": [
        "### Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1S37x6OpJn"
      },
      "source": [
        "#convert sparse labels (i.e., class indices) to one-hot vector labels\n",
        "keras.utils.to_categorical() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs9QFEcePNuj"
      },
      "source": [
        "# plot model\n",
        "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idksvPR2jxl0"
      },
      "source": [
        "# visualize history\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFgmPNiNs9k3"
      },
      "source": [
        "# Save and reload model\n",
        "'''\n",
        "HDF5 format saves both the model’s architecture\n",
        "(including every layer’s hyperparameters) and the values of all the model\n",
        "parameters for every layer (e.g., connection weights and biases). It also\n",
        "saves the optimizer (including its hyperparameters and any state it may\n",
        "have).\n",
        "'''\n",
        "model.save(\"my_keras_model.h5\")\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")\n",
        "\n",
        "# using callbacks\n",
        "\n",
        "# checkpoint callback\n",
        "'''\n",
        "The ModelCheckpoint callback saves checkpoints of your model at regular intervals \n",
        "during training, by default at the end of each epoch.\n",
        "\n",
        "If you use a validation set during training, you can set\n",
        "save_best_only=True when creating the ModelCheckpoint. In this case,\n",
        "it will only save your model when its performance on the validation set is\n",
        "the best so far.\n",
        "'''\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])  \n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb])\n",
        "\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Combine Checkpoint and EarlyStopping callback\n",
        "'''\n",
        "It will interrupt training when it measures no\n",
        "progress on the validation set for a number of epochs (defined by the\n",
        "patience argument), and it will optionally roll back to the best model.\n",
        "\n",
        "You can combine both callbacks to save checkpoints of your model (in\n",
        "case your computer crashes) and interrupt training early when there is no\n",
        "more progress (to avoid wasting time and resources), so no need to use save_best_only.\n",
        "'''\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "mse_test = model.evaluate(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sto2Meu_SNav"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAkfmDeX9Jqq"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tPqcaVkSKu0"
      },
      "source": [
        "# Perceptron\n",
        "'''\n",
        "Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following\n",
        "hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), \n",
        "and penalty=None (no regularization).\n",
        "\n",
        "Contrary to Logistic Regression classifiers, Perceptrons do not\n",
        "output a class probability; rather, they make predictions based on a hard\n",
        "threshold. This is one reason to prefer Logistic Regression over\n",
        "Perceptrons.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
        "y = (iris.target == 0).astype(np.int)\n",
        "\n",
        "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
        "per_clf.fit(X, y)\n",
        "\n",
        "y_pred = per_clf.predict([[2, 0.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gPHwjxKSZYy"
      },
      "source": [
        "# Classification MLP with two hidden layers\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "model = keras.models.Sequential()\n",
        "# Flatten layer convert each input image into a 1D array\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "# model.add(keras.layers.InputLayer(input_shape=(28, 28))) can use this layer to input, too\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "# model.add(keras.layers.Dense(300, activation=keras.activations.relu)) former line is same as this line\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# another way to write the former model\n",
        "model = keras.models.Sequential([\n",
        "                                 layers.Flatten(input_shape=[28, 28]),\n",
        "                                 layers.Dense(300, activation=\"relu\"), \n",
        "                                 layers.Dense(100, activation=\"relu\"),\n",
        "                                 layers.Dense(10, activation=\"softmax\")\n",
        "                                 ])\n",
        "\n",
        "model.summary()\n",
        "weights, biases = model.layers[1].get_weights()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "# history = model.fit(X_train, y_train, epochs=10, validation_split=0.1) use the last 10% of the data (before shuffling) for validation.\n",
        "'''\n",
        "If the training set was very skewed, set the class_weight argument when calling the fit() method, which would\n",
        "give a larger weight to underrepresented classes and a lower weight to overrepresented classes. \n",
        "These weights would be used by Keras when computing the loss.\n",
        "\n",
        "If you need per-instance weights, set the sample_weight argument (it supersedes class_weight). \n",
        "Per-instance weights could be useful if some instances were labeled by experts while\n",
        "others were labeled using a crowdsourcing platform.\n",
        "\n",
        "It’s as simple as calling the fit() method again, since Keras just continues training where it left off.\n",
        "'''\n",
        "model.evaluate(X_test, y_test)\n",
        "y_pred_proba = model.predict(X_test[:3])\n",
        "y_pred_proba.round(2)\n",
        "\n",
        "y_pred_index = np.argmax(model.predict(X_test[:3]), axis=1)\n",
        "y_pred_class = np.array(class_names)[y_pred_index]\n",
        "# Fine-tune\n",
        "'''\n",
        "The first one to check is the learning\n",
        "rate. If that doesn’t help, try another optimizer (and always retune the\n",
        "learning rate after changing any hyperparameter). If the performance is\n",
        "still not great, then try tuning model hyperparameters such as the number\n",
        "of layers, the number of neurons per layer, and the types of activation\n",
        "functions to use for each hidden layer. You can also try tuning other\n",
        "hyperparameters, such as the batch size (it can be set in the fit() method\n",
        "using the batch_size argument, which defaults to 32).\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ejD7lFN9M7Z"
      },
      "source": [
        "#### Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He-u9Xgx9OQF"
      },
      "source": [
        "# regression MLP using Sequential API\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
        "                                 keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_samples = X_test[:3]\n",
        "y_pred = model.predict(X_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2OW_LLDFTMT"
      },
      "source": [
        "# regression MLP using Functional API\n",
        "\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
        "concat = keras.layers.Concatenate()([input_, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKx4f5sQH1ZY"
      },
      "source": [
        "# Wide & Deep neural network\n",
        "# regression MLP using Functional API\n",
        "# with multiple inputs from wide approach & deep approach (may overlapping)\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\") # features 0-4\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\") # features 2-7\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, -6:]\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, -6:]\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, -6:]\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
        "\n",
        "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
        "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
        "\n",
        "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
        "y_pred = model.predict((X_new_A, X_new_B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zFfE0ug65m"
      },
      "source": [
        "#### Multiple outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRZ9UimSIBRn"
      },
      "source": [
        "# Add auxiliary output for regularization\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
        "model = keras.models.Model(inputs=[input_A, input_B],\n",
        "                           outputs=[output, aux_output])\n",
        "\n",
        "# Each output will need its own loss function.\n",
        "'''\n",
        "#We should pass a list of losses (if we pass a single loss, \n",
        "Keras will assume that the same loss must be used for all outputs). By default,\n",
        "Keras will compute all these losses and simply add them up to get the final\n",
        "loss used for training.\n",
        "'''\n",
        "\n",
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "\n",
        "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
        "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
        "\n",
        "total_loss, main_loss, aux_loss = model.evaluate(\n",
        "    [X_test_A, X_test_B], [y_test, y_test])\n",
        "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1A9wvnUsuAL"
      },
      "source": [
        "# Subclassing API\n",
        "'''\n",
        "The big difference is that you can do pretty much anything you want in the call() method: \n",
        "for loops, if statements, low-level TensorFlow operations. This makes it a great API for researchers\n",
        "experimenting with new ideas.\n",
        "\n",
        "This extra flexibility does come at a cost: your model’s architecture is\n",
        "hidden within the call() method, so Keras cannot easily inspect it; it\n",
        "cannot save or clone it; and when you call the summary() method, you\n",
        "only get a list of layers, without any information on how they are\n",
        "connected to each other. Moreover, Keras cannot check types and shapes\n",
        "ahead of time, and it is easier to make mistakes. So unless you really need\n",
        "that extra flexibility, you should probably stick to the Sequential API or\n",
        "the Functional API.\n",
        "\n",
        "Use save_weights() and load_weights()\n",
        "'''\n",
        "\n",
        "class WideAndDeepModel(keras.models.Model):\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
        "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
        "        self.main_output = keras.layers.Dense(1)\n",
        "        self.aux_output = keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        input_A, input_B = inputs\n",
        "        hidden1 = self.hidden1(input_B)\n",
        "        hidden2 = self.hidden2(hidden1)\n",
        "        concat = keras.layers.concatenate([input_A, hidden2])\n",
        "        main_output = self.main_output(concat)\n",
        "        aux_output = self.aux_output(hidden2)\n",
        "        return main_output, aux_output\n",
        "\n",
        "model = WideAndDeepModel(30, activation=\"relu\")\n",
        "\n",
        "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
        "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
        "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
        "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olsrfaFYB52u"
      },
      "source": [
        "### Fine-tune models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRczV3JUB71b"
      },
      "source": [
        "# Randomized Search\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
        "# keras_reg.fit(X_train, y_train, epochs=100,\n",
        "#               validation_data=(X_valid, y_valid),\n",
        "#               callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# mse_test = keras_reg.score(X_test, y_test)\n",
        "# y_pred = keras_reg.predict(X_new)\n",
        "\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "# Note that RandomizedSearchCV uses K-fold crossvalidation, so it does not use \n",
        "# X_valid and y_valid, which are only used for early stopping.\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "rnd_search_cv.best_params_\n",
        "rnd_search_cv.best_score_\n",
        "model = rnd_search_cv.best_estimator_.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15A8XpADueV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}